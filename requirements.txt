torch>=2.1.0
transformers>=4.35.0
datasets>=2.14.0
wandb>=0.16.0
accelerate>=0.24.0
sentencepiece>=0.1.99
huggingface_hub[cli]>=0.19.0

# Optional: For FP8 training on H100+ GPUs (1.5-2x speedup)
# Uncomment the line below if you have H100 or newer GPU:
# transformer-engine>=1.0.0